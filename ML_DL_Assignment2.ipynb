{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3bhfIKbQzGq2"},"source":["# Assignment 2. Music Century Classification\n","\n","For this task, we will construct models to predict the century in which a music piece was released. We will utilize the \"YearPredictionMSD Data Set,\" which is derived from the Million Song Dataset from the UCI Machine Learning Repository. Make sure you download the version of the dataset from the moodle and not from UCI. Here are some relevant links to read on this dataset:\n","\n","- https://archive.ics.uci.edu/ml/datasets/yearpredictionmsd\n","- http://millionsongdataset.com/pages/tasks-demos/#yearrecognition\n","\n","Just like in the last assignment, it is divided to two files.\n","1. This file (ML_DL_Assignment2.ipynb)\n","2. A python functions  file which you will fill out (ML_DL_Functions2.py)\n","\n","As well as the year prediction msd dataset file.\n","\n","In this assignment you will mount and load the dataset and functions file from google drive. To start make sure you have both the template python functions file and the song dataset file(downloaded from the moodle) on the same directory in your google drive.\n","\n","When you are finished with the assignment make sure you submit the following files:\n","1. this file (ML_DL_Assignment2.ipynb).\n","2. the functions file (ML_DL_Functions2.py).\n","3. the weights file from section 2.7 (assignment2_submission_optimal_weights.npy).\n","4. the bias file from section 2.7 (assignment2_submission_optimal_bias.npy).\n","\n","**Make sure you fill your personal ID in the functions file in the return value of function ID1. ID2 is filled only if you were allowed to complete the assignment in pairs otherwise keep it's return value 0.**\n","\n","Note that untill section 2.9 you are not allowed to import additional packages **(especially not PyTorch)**. One of the objectives is to understand how the training procedure actually operates, before working with PyTorch's autograd engine which does it all for us. Importing the pytorch package will deduct from your points."]},{"cell_type":"markdown","metadata":{"id":"47oq1vy5PUIV"},"source":["## 1. Data"]},{"cell_type":"code","metadata":{"id":"1aFWpuNSzGq9","ExecuteTime":{"end_time":"2023-11-08T13:49:56.865016300Z","start_time":"2023-11-08T13:49:56.859011700Z"},"executionInfo":{"status":"ok","timestamp":1708280479644,"user_tz":-120,"elapsed":399,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["import pandas\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sys\n","def reload_functions():\n","  if 'ML_DL_Functions2' in sys.modules:\n","    del sys.modules['ML_DL_Functions2']\n","  functions_path = drive_path.replace(\" \",\"\\ \") + 'ML_DL_Functions2.py'\n","  !cp $functions_path ."],"execution_count":51,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y7UWL6mFzGq-"},"source":["Just like in the last assignment you should mount your google drive and make sure you have both the dataset from the moodle('YearPredictionMSD.csv') and the functions file ('ML_DL_Functions2.py') in the same directory which you will input below:"]},{"cell_type":"code","metadata":{"id":"EY6PrfV4zGq_","ExecuteTime":{"end_time":"2023-11-08T13:50:00.601972300Z","start_time":"2023-11-08T13:50:00.356419700Z"},"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708280493030,"user_tz":-120,"elapsed":11800,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"04d1cb00-e9fe-41fd-fa69-2175fe4230e8"},"source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","drive_path = '/content/gdrive/MyDrive/Intro_to_Machine_Learning/Assignment_2/' # TODO - UPDATE ME WITH THE TRUE PATH!\n","csv_path = drive_path + 'YearPredictionMSD.csv'\n","t_label = [\"year\"]\n","x_labels = [\"var%d\" % i for i in range(1, 91)]\n","df = pandas.read_csv(csv_path)"],"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"KgB83beNzGq_"},"source":["Now that the data is loaded to your Colab notebook, you should be able to display the Pandas\n","DataFrame `df` as a table:"]},{"cell_type":"code","metadata":{"id":"H5bBEnj3zGq_","colab":{"base_uri":"https://localhost:8080/","height":444},"executionInfo":{"status":"ok","timestamp":1708280494425,"user_tz":-120,"elapsed":8,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"5475acb6-6f58-48a7-cdda-9342ff44f08b"},"source":["df"],"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["        year      var1      var2      var3      var4      var5      var6  \\\n","0       2001  49.94357  21.47114  73.07750   8.74861 -17.40628 -13.09905   \n","1       2001  48.73215  18.42930  70.32679  12.94636 -10.32437 -24.83777   \n","2       2001  50.95714  31.85602  55.81851  13.41693  -6.57898 -18.54940   \n","3       2001  48.24750  -1.89837  36.29772   2.58776   0.97170 -26.21683   \n","4       2001  50.97020  42.20998  67.09964   8.46791 -15.85279 -16.81409   \n","...      ...       ...       ...       ...       ...       ...       ...   \n","508710  1979  42.88385 -17.41629 -13.51726  -0.75243   4.74785  14.33437   \n","508711  2010  42.47120  13.16539  -6.89795  14.78750  14.72776 -12.05820   \n","508712  2010  45.21104 -19.03522 -16.50919  19.30722 -22.23290 -25.77296   \n","508713  2004  44.60991  29.26510 -14.79970  16.26654 -20.44287  34.93228   \n","508714  2002  51.58607  62.50479  10.33764  -7.60272 -35.88361 -30.27671   \n","\n","            var7      var8      var9  ...     var81      var82     var83  \\\n","0      -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548  58.99367   \n","1        8.76630  -0.92019  18.76548  ...   5.66812  -19.68073  33.04964   \n","2       -3.27872  -2.35035  16.07017  ...   3.03800   26.05866 -50.92779   \n","3        5.05097 -10.34124   3.55005  ...  34.57337 -171.70734 -16.96705   \n","4      -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724  64.92712   \n","...          ...       ...       ...  ...       ...        ...       ...   \n","508710 -11.75670 -10.46058 -14.91937  ... -23.01045 -169.62524  43.90683   \n","508711  -6.56437  -7.70141  -8.01135  ...   6.97510   97.98602 -45.39312   \n","508712  15.66504  -3.26132   1.78980  ...  17.64373   27.46728  48.64159   \n","508713  -8.15282   2.94035  -1.93460  ...  21.30827 -183.32526 -40.60815   \n","508714   7.36746   0.33782   5.51725  ...   0.57308   34.69757 -22.81646   \n","\n","           var84     var85      var86      var87     var88      var89  \\\n","0       15.37344   1.11144  -23.08793   68.40795  -1.82223  -27.46348   \n","1       42.87836  -9.90378  -32.22788   70.49388  12.04941   58.43453   \n","2       10.93792  -0.07568   43.20130 -115.00698  -0.05859   39.67068   \n","3      -46.67617 -12.51516   82.58061  -72.08993   9.90558  199.62971   \n","4      -17.72522  -1.49237   -7.50035   51.76631   7.88713   55.66926   \n","...          ...       ...        ...        ...       ...        ...   \n","508710  15.45299   2.84499   94.83469 -157.26665   3.60034   54.26775   \n","508711 -30.26953  -9.49116  -51.58060  -12.08770   0.10696  117.82374   \n","508712  92.03877  11.31597 -189.77886  179.06219  -3.74635  -27.01421   \n","508713  19.53727  12.13429 -133.10456 -158.46478  22.36919  161.58392   \n","508714  27.20861  -1.22381   -8.51102  -18.26440   2.22031  -93.94392   \n","\n","           var90  \n","0        2.26327  \n","1       26.92061  \n","2       -0.66345  \n","3       18.85382  \n","4       28.74903  \n","...          ...  \n","508710 -22.24375  \n","508711  -1.06577  \n","508712 -10.23084  \n","508713 -18.54131  \n","508714  -6.75431  \n","\n","[508715 rows x 91 columns]"],"text/html":["\n","  <div id=\"df-f01ce5e3-cc4b-4e30-8778-46bb9705705a\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>var1</th>\n","      <th>var2</th>\n","      <th>var3</th>\n","      <th>var4</th>\n","      <th>var5</th>\n","      <th>var6</th>\n","      <th>var7</th>\n","      <th>var8</th>\n","      <th>var9</th>\n","      <th>...</th>\n","      <th>var81</th>\n","      <th>var82</th>\n","      <th>var83</th>\n","      <th>var84</th>\n","      <th>var85</th>\n","      <th>var86</th>\n","      <th>var87</th>\n","      <th>var88</th>\n","      <th>var89</th>\n","      <th>var90</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2001</td>\n","      <td>49.94357</td>\n","      <td>21.47114</td>\n","      <td>73.07750</td>\n","      <td>8.74861</td>\n","      <td>-17.40628</td>\n","      <td>-13.09905</td>\n","      <td>-25.01202</td>\n","      <td>-12.23257</td>\n","      <td>7.83089</td>\n","      <td>...</td>\n","      <td>13.01620</td>\n","      <td>-54.40548</td>\n","      <td>58.99367</td>\n","      <td>15.37344</td>\n","      <td>1.11144</td>\n","      <td>-23.08793</td>\n","      <td>68.40795</td>\n","      <td>-1.82223</td>\n","      <td>-27.46348</td>\n","      <td>2.26327</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2001</td>\n","      <td>48.73215</td>\n","      <td>18.42930</td>\n","      <td>70.32679</td>\n","      <td>12.94636</td>\n","      <td>-10.32437</td>\n","      <td>-24.83777</td>\n","      <td>8.76630</td>\n","      <td>-0.92019</td>\n","      <td>18.76548</td>\n","      <td>...</td>\n","      <td>5.66812</td>\n","      <td>-19.68073</td>\n","      <td>33.04964</td>\n","      <td>42.87836</td>\n","      <td>-9.90378</td>\n","      <td>-32.22788</td>\n","      <td>70.49388</td>\n","      <td>12.04941</td>\n","      <td>58.43453</td>\n","      <td>26.92061</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2001</td>\n","      <td>50.95714</td>\n","      <td>31.85602</td>\n","      <td>55.81851</td>\n","      <td>13.41693</td>\n","      <td>-6.57898</td>\n","      <td>-18.54940</td>\n","      <td>-3.27872</td>\n","      <td>-2.35035</td>\n","      <td>16.07017</td>\n","      <td>...</td>\n","      <td>3.03800</td>\n","      <td>26.05866</td>\n","      <td>-50.92779</td>\n","      <td>10.93792</td>\n","      <td>-0.07568</td>\n","      <td>43.20130</td>\n","      <td>-115.00698</td>\n","      <td>-0.05859</td>\n","      <td>39.67068</td>\n","      <td>-0.66345</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2001</td>\n","      <td>48.24750</td>\n","      <td>-1.89837</td>\n","      <td>36.29772</td>\n","      <td>2.58776</td>\n","      <td>0.97170</td>\n","      <td>-26.21683</td>\n","      <td>5.05097</td>\n","      <td>-10.34124</td>\n","      <td>3.55005</td>\n","      <td>...</td>\n","      <td>34.57337</td>\n","      <td>-171.70734</td>\n","      <td>-16.96705</td>\n","      <td>-46.67617</td>\n","      <td>-12.51516</td>\n","      <td>82.58061</td>\n","      <td>-72.08993</td>\n","      <td>9.90558</td>\n","      <td>199.62971</td>\n","      <td>18.85382</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2001</td>\n","      <td>50.97020</td>\n","      <td>42.20998</td>\n","      <td>67.09964</td>\n","      <td>8.46791</td>\n","      <td>-15.85279</td>\n","      <td>-16.81409</td>\n","      <td>-12.48207</td>\n","      <td>-9.37636</td>\n","      <td>12.63699</td>\n","      <td>...</td>\n","      <td>9.92661</td>\n","      <td>-55.95724</td>\n","      <td>64.92712</td>\n","      <td>-17.72522</td>\n","      <td>-1.49237</td>\n","      <td>-7.50035</td>\n","      <td>51.76631</td>\n","      <td>7.88713</td>\n","      <td>55.66926</td>\n","      <td>28.74903</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>508710</th>\n","      <td>1979</td>\n","      <td>42.88385</td>\n","      <td>-17.41629</td>\n","      <td>-13.51726</td>\n","      <td>-0.75243</td>\n","      <td>4.74785</td>\n","      <td>14.33437</td>\n","      <td>-11.75670</td>\n","      <td>-10.46058</td>\n","      <td>-14.91937</td>\n","      <td>...</td>\n","      <td>-23.01045</td>\n","      <td>-169.62524</td>\n","      <td>43.90683</td>\n","      <td>15.45299</td>\n","      <td>2.84499</td>\n","      <td>94.83469</td>\n","      <td>-157.26665</td>\n","      <td>3.60034</td>\n","      <td>54.26775</td>\n","      <td>-22.24375</td>\n","    </tr>\n","    <tr>\n","      <th>508711</th>\n","      <td>2010</td>\n","      <td>42.47120</td>\n","      <td>13.16539</td>\n","      <td>-6.89795</td>\n","      <td>14.78750</td>\n","      <td>14.72776</td>\n","      <td>-12.05820</td>\n","      <td>-6.56437</td>\n","      <td>-7.70141</td>\n","      <td>-8.01135</td>\n","      <td>...</td>\n","      <td>6.97510</td>\n","      <td>97.98602</td>\n","      <td>-45.39312</td>\n","      <td>-30.26953</td>\n","      <td>-9.49116</td>\n","      <td>-51.58060</td>\n","      <td>-12.08770</td>\n","      <td>0.10696</td>\n","      <td>117.82374</td>\n","      <td>-1.06577</td>\n","    </tr>\n","    <tr>\n","      <th>508712</th>\n","      <td>2010</td>\n","      <td>45.21104</td>\n","      <td>-19.03522</td>\n","      <td>-16.50919</td>\n","      <td>19.30722</td>\n","      <td>-22.23290</td>\n","      <td>-25.77296</td>\n","      <td>15.66504</td>\n","      <td>-3.26132</td>\n","      <td>1.78980</td>\n","      <td>...</td>\n","      <td>17.64373</td>\n","      <td>27.46728</td>\n","      <td>48.64159</td>\n","      <td>92.03877</td>\n","      <td>11.31597</td>\n","      <td>-189.77886</td>\n","      <td>179.06219</td>\n","      <td>-3.74635</td>\n","      <td>-27.01421</td>\n","      <td>-10.23084</td>\n","    </tr>\n","    <tr>\n","      <th>508713</th>\n","      <td>2004</td>\n","      <td>44.60991</td>\n","      <td>29.26510</td>\n","      <td>-14.79970</td>\n","      <td>16.26654</td>\n","      <td>-20.44287</td>\n","      <td>34.93228</td>\n","      <td>-8.15282</td>\n","      <td>2.94035</td>\n","      <td>-1.93460</td>\n","      <td>...</td>\n","      <td>21.30827</td>\n","      <td>-183.32526</td>\n","      <td>-40.60815</td>\n","      <td>19.53727</td>\n","      <td>12.13429</td>\n","      <td>-133.10456</td>\n","      <td>-158.46478</td>\n","      <td>22.36919</td>\n","      <td>161.58392</td>\n","      <td>-18.54131</td>\n","    </tr>\n","    <tr>\n","      <th>508714</th>\n","      <td>2002</td>\n","      <td>51.58607</td>\n","      <td>62.50479</td>\n","      <td>10.33764</td>\n","      <td>-7.60272</td>\n","      <td>-35.88361</td>\n","      <td>-30.27671</td>\n","      <td>7.36746</td>\n","      <td>0.33782</td>\n","      <td>5.51725</td>\n","      <td>...</td>\n","      <td>0.57308</td>\n","      <td>34.69757</td>\n","      <td>-22.81646</td>\n","      <td>27.20861</td>\n","      <td>-1.22381</td>\n","      <td>-8.51102</td>\n","      <td>-18.26440</td>\n","      <td>2.22031</td>\n","      <td>-93.94392</td>\n","      <td>-6.75431</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>508715 rows × 91 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f01ce5e3-cc4b-4e30-8778-46bb9705705a')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-f01ce5e3-cc4b-4e30-8778-46bb9705705a button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-f01ce5e3-cc4b-4e30-8778-46bb9705705a');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-78291def-863c-4b12-9183-2b5510a8cee8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-78291def-863c-4b12-9183-2b5510a8cee8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-78291def-863c-4b12-9183-2b5510a8cee8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"KaLuAMH_zGrA"},"source":["To set up our data for classification, we'll use the \"year\" field to represent\n","whether a song was released in the 20-th century. In our case `df[\"year\"]` will be 1 if\n","the year was released after 2000, and 0 otherwise."]},{"cell_type":"code","metadata":{"id":"tZdGlNgdzGrA","executionInfo":{"status":"ok","timestamp":1708280495996,"user_tz":-120,"elapsed":4,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["df[\"year\"] = df[\"year\"].map(lambda x: int(x > 2000))"],"execution_count":54,"outputs":[]},{"cell_type":"code","metadata":{"id":"xugy7FZ8eoAd","colab":{"base_uri":"https://localhost:8080/","height":726},"executionInfo":{"status":"ok","timestamp":1708280496607,"user_tz":-120,"elapsed":6,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"6e25dbb1-d071-4744-8879-c0012dbbf3ff"},"source":["df.head(20)"],"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    year      var1       var2      var3      var4      var5      var6  \\\n","0      1  49.94357   21.47114  73.07750   8.74861 -17.40628 -13.09905   \n","1      1  48.73215   18.42930  70.32679  12.94636 -10.32437 -24.83777   \n","2      1  50.95714   31.85602  55.81851  13.41693  -6.57898 -18.54940   \n","3      1  48.24750   -1.89837  36.29772   2.58776   0.97170 -26.21683   \n","4      1  50.97020   42.20998  67.09964   8.46791 -15.85279 -16.81409   \n","5      1  50.54767    0.31568  92.35066  22.38696 -25.51870 -19.04928   \n","6      1  50.57546   33.17843  50.53517  11.55217 -27.24764  -8.78206   \n","7      1  48.26892    8.97526  75.23158  24.04945 -16.02105 -14.09491   \n","8      1  49.75468   33.99581  56.73846   2.89581  -2.92429 -26.44413   \n","9      1  45.17809   46.34234 -40.65357  -2.47909   1.21253  -0.65302   \n","10     1  39.13076  -23.01763 -36.20583   1.67519  -4.27101  13.01158   \n","11     1  37.66498  -34.05910 -17.36060 -26.77781 -39.95119 -20.75000   \n","12     1  26.51957 -148.15762 -13.30095  -7.25851  17.22029 -21.99439   \n","13     1  37.68491  -26.84185 -27.10566 -14.95883  -5.87200 -21.68979   \n","14     0  39.11695   -8.29767 -51.37966  -4.42668 -30.06506 -11.95916   \n","15     1  35.05129  -67.97714 -14.20239  -6.68696  -0.61230 -18.70341   \n","16     1  33.63129  -96.14912 -89.38216 -12.11699  13.77252  -6.69377   \n","17     0  41.38639  -20.78665  51.80155  17.21415 -36.44189 -11.53169   \n","18     0  37.45034   11.42615  56.28982  19.58426 -16.43530   2.22457   \n","19     0  39.71092   -4.92800  12.88590 -11.87773   2.48031 -16.11028   \n","\n","        var7      var8      var9  ...     var81      var82      var83  \\\n","0  -25.01202 -12.23257   7.83089  ...  13.01620  -54.40548   58.99367   \n","1    8.76630  -0.92019  18.76548  ...   5.66812  -19.68073   33.04964   \n","2   -3.27872  -2.35035  16.07017  ...   3.03800   26.05866  -50.92779   \n","3    5.05097 -10.34124   3.55005  ...  34.57337 -171.70734  -16.96705   \n","4  -12.48207  -9.37636  12.63699  ...   9.92661  -55.95724   64.92712   \n","5   20.67345  -5.19943   3.63566  ...   6.59753  -50.69577   26.02574   \n","6  -12.04282  -9.53930  28.61811  ...  11.63681   25.44182  134.62382   \n","7    8.11871  -1.87566   7.46701  ...  18.03989  -58.46192  -65.56438   \n","8    1.71392  -0.55644  22.08594  ...  18.70812    5.20391  -27.75192   \n","9   -6.95536 -12.20040  17.02512  ...  -4.36742  -87.55285  -70.79677   \n","10   8.05718  -8.41088   6.27370  ...  32.86051  -26.08461 -186.82429   \n","11  -0.10231  -0.89972  -1.30205  ...  11.18909   45.20614   53.83925   \n","12   5.51947   3.48418   2.61738  ...  23.80442  251.76360   18.81642   \n","13   4.87374 -18.01800   1.52141  ... -67.57637  234.27192  -72.34557   \n","14  -0.85322  -8.86179  11.36680  ...  42.22923  478.26580  -10.33823   \n","15  -1.31928  -9.46370   5.53492  ...  10.25585   94.90539   15.95689   \n","16 -33.36843 -24.81437  21.22757  ...  49.93249  -14.47489   40.70590   \n","17  11.75252  -7.62428  -3.65488  ...  50.37614  -40.48205   48.07805   \n","18   1.02668  -7.34736  -0.01184  ... -22.46207  -25.77228 -322.42841   \n","19 -16.40421  -8.29657   9.86817  ...  11.92816  -73.72412   16.19039   \n","\n","        var84     var85      var86      var87     var88       var89     var90  \n","0    15.37344   1.11144  -23.08793   68.40795  -1.82223   -27.46348   2.26327  \n","1    42.87836  -9.90378  -32.22788   70.49388  12.04941    58.43453  26.92061  \n","2    10.93792  -0.07568   43.20130 -115.00698  -0.05859    39.67068  -0.66345  \n","3   -46.67617 -12.51516   82.58061  -72.08993   9.90558   199.62971  18.85382  \n","4   -17.72522  -1.49237   -7.50035   51.76631   7.88713    55.66926  28.74903  \n","5    18.94430  -0.33730    6.09352   35.18381   5.00283   -11.02257   0.02263  \n","6    21.51982   8.17570   35.46251   11.57736   4.50056    -4.62739   1.40192  \n","7    46.99856  -4.09602   56.37650  -18.29975  -0.30633     3.98364  -3.72556  \n","8    17.22100  -0.85210  -15.67150  -26.36257   5.48708    -9.13495   6.08680  \n","9    76.57355  -7.71727    3.26926 -298.49845  11.49326   -89.21804 -15.09719  \n","10  113.58176   9.28727   44.60282  158.00425  -2.59543   109.19723  23.36143  \n","11    2.59467  -4.00958  -47.74886 -170.92864  -5.19009     8.83617  -7.16056  \n","12  157.09656 -27.79449 -137.72740  115.28414  23.00230  -164.02536  51.54138  \n","13 -362.25101 -25.55019  -89.08971 -891.58937  14.11648 -1030.99180  99.28967  \n","14 -103.76858  39.19511  -98.76636 -122.81061  -2.14942  -211.48202 -12.81569  \n","15  -98.15732  -9.64859  -93.52834  -95.82981  20.73063  -562.07671  43.44696  \n","16   58.63692   8.81522   27.28474    5.78046   3.44539   259.10825  10.28525  \n","17   -7.62399   6.51934  -30.46090  -53.87264   4.44627    58.16913  -0.02409  \n","18 -146.57408  13.61588   92.22918 -439.80259  25.73235   157.22967  38.70617  \n","19    9.79606   9.71693   -9.90907  -20.65851   2.34002   -31.57015   1.58400  \n","\n","[20 rows x 91 columns]"],"text/html":["\n","  <div id=\"df-4ed891db-61f7-4704-b2e7-e017fb487066\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>year</th>\n","      <th>var1</th>\n","      <th>var2</th>\n","      <th>var3</th>\n","      <th>var4</th>\n","      <th>var5</th>\n","      <th>var6</th>\n","      <th>var7</th>\n","      <th>var8</th>\n","      <th>var9</th>\n","      <th>...</th>\n","      <th>var81</th>\n","      <th>var82</th>\n","      <th>var83</th>\n","      <th>var84</th>\n","      <th>var85</th>\n","      <th>var86</th>\n","      <th>var87</th>\n","      <th>var88</th>\n","      <th>var89</th>\n","      <th>var90</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>49.94357</td>\n","      <td>21.47114</td>\n","      <td>73.07750</td>\n","      <td>8.74861</td>\n","      <td>-17.40628</td>\n","      <td>-13.09905</td>\n","      <td>-25.01202</td>\n","      <td>-12.23257</td>\n","      <td>7.83089</td>\n","      <td>...</td>\n","      <td>13.01620</td>\n","      <td>-54.40548</td>\n","      <td>58.99367</td>\n","      <td>15.37344</td>\n","      <td>1.11144</td>\n","      <td>-23.08793</td>\n","      <td>68.40795</td>\n","      <td>-1.82223</td>\n","      <td>-27.46348</td>\n","      <td>2.26327</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>48.73215</td>\n","      <td>18.42930</td>\n","      <td>70.32679</td>\n","      <td>12.94636</td>\n","      <td>-10.32437</td>\n","      <td>-24.83777</td>\n","      <td>8.76630</td>\n","      <td>-0.92019</td>\n","      <td>18.76548</td>\n","      <td>...</td>\n","      <td>5.66812</td>\n","      <td>-19.68073</td>\n","      <td>33.04964</td>\n","      <td>42.87836</td>\n","      <td>-9.90378</td>\n","      <td>-32.22788</td>\n","      <td>70.49388</td>\n","      <td>12.04941</td>\n","      <td>58.43453</td>\n","      <td>26.92061</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>50.95714</td>\n","      <td>31.85602</td>\n","      <td>55.81851</td>\n","      <td>13.41693</td>\n","      <td>-6.57898</td>\n","      <td>-18.54940</td>\n","      <td>-3.27872</td>\n","      <td>-2.35035</td>\n","      <td>16.07017</td>\n","      <td>...</td>\n","      <td>3.03800</td>\n","      <td>26.05866</td>\n","      <td>-50.92779</td>\n","      <td>10.93792</td>\n","      <td>-0.07568</td>\n","      <td>43.20130</td>\n","      <td>-115.00698</td>\n","      <td>-0.05859</td>\n","      <td>39.67068</td>\n","      <td>-0.66345</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>48.24750</td>\n","      <td>-1.89837</td>\n","      <td>36.29772</td>\n","      <td>2.58776</td>\n","      <td>0.97170</td>\n","      <td>-26.21683</td>\n","      <td>5.05097</td>\n","      <td>-10.34124</td>\n","      <td>3.55005</td>\n","      <td>...</td>\n","      <td>34.57337</td>\n","      <td>-171.70734</td>\n","      <td>-16.96705</td>\n","      <td>-46.67617</td>\n","      <td>-12.51516</td>\n","      <td>82.58061</td>\n","      <td>-72.08993</td>\n","      <td>9.90558</td>\n","      <td>199.62971</td>\n","      <td>18.85382</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>50.97020</td>\n","      <td>42.20998</td>\n","      <td>67.09964</td>\n","      <td>8.46791</td>\n","      <td>-15.85279</td>\n","      <td>-16.81409</td>\n","      <td>-12.48207</td>\n","      <td>-9.37636</td>\n","      <td>12.63699</td>\n","      <td>...</td>\n","      <td>9.92661</td>\n","      <td>-55.95724</td>\n","      <td>64.92712</td>\n","      <td>-17.72522</td>\n","      <td>-1.49237</td>\n","      <td>-7.50035</td>\n","      <td>51.76631</td>\n","      <td>7.88713</td>\n","      <td>55.66926</td>\n","      <td>28.74903</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>50.54767</td>\n","      <td>0.31568</td>\n","      <td>92.35066</td>\n","      <td>22.38696</td>\n","      <td>-25.51870</td>\n","      <td>-19.04928</td>\n","      <td>20.67345</td>\n","      <td>-5.19943</td>\n","      <td>3.63566</td>\n","      <td>...</td>\n","      <td>6.59753</td>\n","      <td>-50.69577</td>\n","      <td>26.02574</td>\n","      <td>18.94430</td>\n","      <td>-0.33730</td>\n","      <td>6.09352</td>\n","      <td>35.18381</td>\n","      <td>5.00283</td>\n","      <td>-11.02257</td>\n","      <td>0.02263</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>50.57546</td>\n","      <td>33.17843</td>\n","      <td>50.53517</td>\n","      <td>11.55217</td>\n","      <td>-27.24764</td>\n","      <td>-8.78206</td>\n","      <td>-12.04282</td>\n","      <td>-9.53930</td>\n","      <td>28.61811</td>\n","      <td>...</td>\n","      <td>11.63681</td>\n","      <td>25.44182</td>\n","      <td>134.62382</td>\n","      <td>21.51982</td>\n","      <td>8.17570</td>\n","      <td>35.46251</td>\n","      <td>11.57736</td>\n","      <td>4.50056</td>\n","      <td>-4.62739</td>\n","      <td>1.40192</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>48.26892</td>\n","      <td>8.97526</td>\n","      <td>75.23158</td>\n","      <td>24.04945</td>\n","      <td>-16.02105</td>\n","      <td>-14.09491</td>\n","      <td>8.11871</td>\n","      <td>-1.87566</td>\n","      <td>7.46701</td>\n","      <td>...</td>\n","      <td>18.03989</td>\n","      <td>-58.46192</td>\n","      <td>-65.56438</td>\n","      <td>46.99856</td>\n","      <td>-4.09602</td>\n","      <td>56.37650</td>\n","      <td>-18.29975</td>\n","      <td>-0.30633</td>\n","      <td>3.98364</td>\n","      <td>-3.72556</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>49.75468</td>\n","      <td>33.99581</td>\n","      <td>56.73846</td>\n","      <td>2.89581</td>\n","      <td>-2.92429</td>\n","      <td>-26.44413</td>\n","      <td>1.71392</td>\n","      <td>-0.55644</td>\n","      <td>22.08594</td>\n","      <td>...</td>\n","      <td>18.70812</td>\n","      <td>5.20391</td>\n","      <td>-27.75192</td>\n","      <td>17.22100</td>\n","      <td>-0.85210</td>\n","      <td>-15.67150</td>\n","      <td>-26.36257</td>\n","      <td>5.48708</td>\n","      <td>-9.13495</td>\n","      <td>6.08680</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>45.17809</td>\n","      <td>46.34234</td>\n","      <td>-40.65357</td>\n","      <td>-2.47909</td>\n","      <td>1.21253</td>\n","      <td>-0.65302</td>\n","      <td>-6.95536</td>\n","      <td>-12.20040</td>\n","      <td>17.02512</td>\n","      <td>...</td>\n","      <td>-4.36742</td>\n","      <td>-87.55285</td>\n","      <td>-70.79677</td>\n","      <td>76.57355</td>\n","      <td>-7.71727</td>\n","      <td>3.26926</td>\n","      <td>-298.49845</td>\n","      <td>11.49326</td>\n","      <td>-89.21804</td>\n","      <td>-15.09719</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>39.13076</td>\n","      <td>-23.01763</td>\n","      <td>-36.20583</td>\n","      <td>1.67519</td>\n","      <td>-4.27101</td>\n","      <td>13.01158</td>\n","      <td>8.05718</td>\n","      <td>-8.41088</td>\n","      <td>6.27370</td>\n","      <td>...</td>\n","      <td>32.86051</td>\n","      <td>-26.08461</td>\n","      <td>-186.82429</td>\n","      <td>113.58176</td>\n","      <td>9.28727</td>\n","      <td>44.60282</td>\n","      <td>158.00425</td>\n","      <td>-2.59543</td>\n","      <td>109.19723</td>\n","      <td>23.36143</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>37.66498</td>\n","      <td>-34.05910</td>\n","      <td>-17.36060</td>\n","      <td>-26.77781</td>\n","      <td>-39.95119</td>\n","      <td>-20.75000</td>\n","      <td>-0.10231</td>\n","      <td>-0.89972</td>\n","      <td>-1.30205</td>\n","      <td>...</td>\n","      <td>11.18909</td>\n","      <td>45.20614</td>\n","      <td>53.83925</td>\n","      <td>2.59467</td>\n","      <td>-4.00958</td>\n","      <td>-47.74886</td>\n","      <td>-170.92864</td>\n","      <td>-5.19009</td>\n","      <td>8.83617</td>\n","      <td>-7.16056</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>26.51957</td>\n","      <td>-148.15762</td>\n","      <td>-13.30095</td>\n","      <td>-7.25851</td>\n","      <td>17.22029</td>\n","      <td>-21.99439</td>\n","      <td>5.51947</td>\n","      <td>3.48418</td>\n","      <td>2.61738</td>\n","      <td>...</td>\n","      <td>23.80442</td>\n","      <td>251.76360</td>\n","      <td>18.81642</td>\n","      <td>157.09656</td>\n","      <td>-27.79449</td>\n","      <td>-137.72740</td>\n","      <td>115.28414</td>\n","      <td>23.00230</td>\n","      <td>-164.02536</td>\n","      <td>51.54138</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>37.68491</td>\n","      <td>-26.84185</td>\n","      <td>-27.10566</td>\n","      <td>-14.95883</td>\n","      <td>-5.87200</td>\n","      <td>-21.68979</td>\n","      <td>4.87374</td>\n","      <td>-18.01800</td>\n","      <td>1.52141</td>\n","      <td>...</td>\n","      <td>-67.57637</td>\n","      <td>234.27192</td>\n","      <td>-72.34557</td>\n","      <td>-362.25101</td>\n","      <td>-25.55019</td>\n","      <td>-89.08971</td>\n","      <td>-891.58937</td>\n","      <td>14.11648</td>\n","      <td>-1030.99180</td>\n","      <td>99.28967</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0</td>\n","      <td>39.11695</td>\n","      <td>-8.29767</td>\n","      <td>-51.37966</td>\n","      <td>-4.42668</td>\n","      <td>-30.06506</td>\n","      <td>-11.95916</td>\n","      <td>-0.85322</td>\n","      <td>-8.86179</td>\n","      <td>11.36680</td>\n","      <td>...</td>\n","      <td>42.22923</td>\n","      <td>478.26580</td>\n","      <td>-10.33823</td>\n","      <td>-103.76858</td>\n","      <td>39.19511</td>\n","      <td>-98.76636</td>\n","      <td>-122.81061</td>\n","      <td>-2.14942</td>\n","      <td>-211.48202</td>\n","      <td>-12.81569</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>35.05129</td>\n","      <td>-67.97714</td>\n","      <td>-14.20239</td>\n","      <td>-6.68696</td>\n","      <td>-0.61230</td>\n","      <td>-18.70341</td>\n","      <td>-1.31928</td>\n","      <td>-9.46370</td>\n","      <td>5.53492</td>\n","      <td>...</td>\n","      <td>10.25585</td>\n","      <td>94.90539</td>\n","      <td>15.95689</td>\n","      <td>-98.15732</td>\n","      <td>-9.64859</td>\n","      <td>-93.52834</td>\n","      <td>-95.82981</td>\n","      <td>20.73063</td>\n","      <td>-562.07671</td>\n","      <td>43.44696</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>33.63129</td>\n","      <td>-96.14912</td>\n","      <td>-89.38216</td>\n","      <td>-12.11699</td>\n","      <td>13.77252</td>\n","      <td>-6.69377</td>\n","      <td>-33.36843</td>\n","      <td>-24.81437</td>\n","      <td>21.22757</td>\n","      <td>...</td>\n","      <td>49.93249</td>\n","      <td>-14.47489</td>\n","      <td>40.70590</td>\n","      <td>58.63692</td>\n","      <td>8.81522</td>\n","      <td>27.28474</td>\n","      <td>5.78046</td>\n","      <td>3.44539</td>\n","      <td>259.10825</td>\n","      <td>10.28525</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0</td>\n","      <td>41.38639</td>\n","      <td>-20.78665</td>\n","      <td>51.80155</td>\n","      <td>17.21415</td>\n","      <td>-36.44189</td>\n","      <td>-11.53169</td>\n","      <td>11.75252</td>\n","      <td>-7.62428</td>\n","      <td>-3.65488</td>\n","      <td>...</td>\n","      <td>50.37614</td>\n","      <td>-40.48205</td>\n","      <td>48.07805</td>\n","      <td>-7.62399</td>\n","      <td>6.51934</td>\n","      <td>-30.46090</td>\n","      <td>-53.87264</td>\n","      <td>4.44627</td>\n","      <td>58.16913</td>\n","      <td>-0.02409</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0</td>\n","      <td>37.45034</td>\n","      <td>11.42615</td>\n","      <td>56.28982</td>\n","      <td>19.58426</td>\n","      <td>-16.43530</td>\n","      <td>2.22457</td>\n","      <td>1.02668</td>\n","      <td>-7.34736</td>\n","      <td>-0.01184</td>\n","      <td>...</td>\n","      <td>-22.46207</td>\n","      <td>-25.77228</td>\n","      <td>-322.42841</td>\n","      <td>-146.57408</td>\n","      <td>13.61588</td>\n","      <td>92.22918</td>\n","      <td>-439.80259</td>\n","      <td>25.73235</td>\n","      <td>157.22967</td>\n","      <td>38.70617</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0</td>\n","      <td>39.71092</td>\n","      <td>-4.92800</td>\n","      <td>12.88590</td>\n","      <td>-11.87773</td>\n","      <td>2.48031</td>\n","      <td>-16.11028</td>\n","      <td>-16.40421</td>\n","      <td>-8.29657</td>\n","      <td>9.86817</td>\n","      <td>...</td>\n","      <td>11.92816</td>\n","      <td>-73.72412</td>\n","      <td>16.19039</td>\n","      <td>9.79606</td>\n","      <td>9.71693</td>\n","      <td>-9.90907</td>\n","      <td>-20.65851</td>\n","      <td>2.34002</td>\n","      <td>-31.57015</td>\n","      <td>1.58400</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>20 rows × 91 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4ed891db-61f7-4704-b2e7-e017fb487066')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-4ed891db-61f7-4704-b2e7-e017fb487066 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-4ed891db-61f7-4704-b2e7-e017fb487066');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-7ec85fff-8b2b-4cad-afb7-aefe3f4df30c\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ec85fff-8b2b-4cad-afb7-aefe3f4df30c')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-7ec85fff-8b2b-4cad-afb7-aefe3f4df30c button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"df"}},"metadata":{},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"ncjxI4WdzGrA"},"source":["### 1.1 - Train Test Split\n","\n","The data set description text asks us to respect the below train/test split to\n","avoid the \"producer effect\". That is, we want to make sure that no song from a single artist\n","ends up in both the training and test set.\n","\n","#### Food for thought:\n","why would it be problematic to have some songs from an artist in the training set, and other songs from the same artist in the test set. (Hint: Remember that we want our test accuracy to predict how well the model will perform in practice on a song it hasn't learned about.)"]},{"cell_type":"code","metadata":{"id":"2NiYlxpFzGrB","executionInfo":{"status":"ok","timestamp":1708280676643,"user_tz":-120,"elapsed":410,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["# train test split\n","df_train = df[:463715]\n","df_test = df[463715:]\n","\n","# convert to numpy\n","train_xs = df_train[x_labels].to_numpy()\n","train_ts = df_train[t_label].to_numpy()\n","test_xs = df_test[x_labels].to_numpy()\n","test_ts = df_test[t_label].to_numpy()"],"execution_count":68,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BYSzd4XUzGrB"},"source":["### Part (b) -- 7%\n","Normalize the data by subtracting the mean and dividing by the std just like the last assignment."]},{"cell_type":"code","metadata":{"id":"TPuWLksJzGrB","executionInfo":{"status":"ok","timestamp":1708280677666,"user_tz":-120,"elapsed":446,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["# Insert your code here:\n","feature_means = np.mean(train_xs, axis=0)\n","feature_stds  = np.std(train_xs, axis=0)\n","feature_stds[feature_stds==0] = 0.01\n","train_norm_xs = (train_xs - feature_means) / feature_stds\n","test_norm_xs = (test_xs - feature_means) / feature_stds"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k4GqL5J_zGrC"},"source":["### Part (c) -- 7%\n","\n","Finally, we'll move some of the data in our training set into a validation set.\n","#### Food for thought:\n","Why should we limit how many times we use the test set, and how do we use the validation set during the model building process?"]},{"cell_type":"code","metadata":{"id":"HsXv1U3gzGrC","executionInfo":{"status":"ok","timestamp":1708280680453,"user_tz":-120,"elapsed":1687,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["# shuffle the training set\n","reindex = np.random.permutation(len(train_xs))\n","train_xs = train_xs[reindex]\n","train_norm_xs = train_norm_xs[reindex]\n","train_ts = train_ts[reindex]\n","\n","# use the first 50000 elements of `train_xs` as the validation set\n","train_xs, val_xs           = train_xs[50000:], train_xs[:50000]\n","train_norm_xs, val_norm_xs = train_norm_xs[50000:], train_norm_xs[:50000]\n","train_ts, val_ts           = train_ts[50000:], train_ts[:50000]"],"execution_count":70,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gy4lt445zGrD"},"source":["## Part 2. Classification (79%)\n","\n","We will first build a *classification* model to perform decade classification. We have written a few helper functions for you. You can find them in your functions file ('sigmoid', 'cross_entropy' and 'get_accuracy'). All other code that you write in this section should be vectorized whenever possible (i.e., avoid unnecessary loops). Feel free to add more testing to the notebook to validate your code in the functions file."]},{"cell_type":"markdown","metadata":{"id":"e8ZIfooBzGrD"},"source":["### 2.1 Prediction\n","\n","Fill in the function `pred` in the functions file that computes the prediction `y` based on logistic regression, i.e., a single layer with weights `w` and bias `b`. The output is given by:\n","\\begin{equation}\n","y = \\sigma({\\bf w}^T {\\bf x} + b),\n","\\end{equation}\n","where the value of $y$ is an estimate of the probability that the song is released in the current century, namely ${\\rm year} =1$."]},{"cell_type":"code","metadata":{"id":"naY5mT4_zGrD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708280681409,"user_tz":-120,"elapsed":429,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"6e33f117-5df4-4144-abbe-eef8ba8f027d"},"source":["reload_functions()\n","import ML_DL_Functions2\n","ML_DL_Functions2.pred(np.zeros(90), 1, np.ones([2, 90]))"],"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.73105858, 0.73105858])"]},"metadata":{},"execution_count":71}]},{"cell_type":"markdown","source":[],"metadata":{"id":"xCrT4b0jlxhI"}},{"cell_type":"markdown","source":["### 2.2 Cost\n","Assuming the loss function is the cross entropy function fill in the cost(risk) function in the functions file which returns the mean of the loss function on all inputs.\n","$$\\mathcal{L}_\\mathcal{P}(\\text{Cross Entropy}) = \\mathbb{E}_{(y,t)\\sim\\mathcal{P}}\\left\\{\\text{CE}(t,s)\\right\\}$$\n"],"metadata":{"id":"-fOu-ekEJ0dn"}},{"cell_type":"code","source":["reload_functions()\n","import ML_DL_Functions2\n","print(ML_DL_Functions2.cost(0.5*np.ones(4), np.ones(4)))"],"metadata":{"id":"XmWuoJocXFe4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708280683999,"user_tz":-120,"elapsed":430,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"25689025-f560-4048-b90d-7ea32cbecd9d"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6931471805599453\n"]}]},{"cell_type":"markdown","metadata":{"id":"bxNdmSd3zGrE"},"source":["### 2.3 Derivative of the cost -- 7%\n","Take a pen and paper and calculate the analytical derivative of the cost function with respect to the weights and bias. use the formula calculated to fill in the function `derivative_cost` that computes and returns the gradients\n","$\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$ and\n","$\\frac{\\partial\\mathcal{L}}{\\partial b}$. Here, `X` is the input, `y` is the prediction, and `t` is the true label.\n"]},{"cell_type":"code","metadata":{"id":"P80bu7qmzGrE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708280685788,"user_tz":-120,"elapsed":3,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"b65d093e-cd0b-43c8-ff9d-c31077b3fa57"},"source":["reload_functions()\n","import ML_DL_Functions2\n","dldw, dldb = ML_DL_Functions2.derivative_cost(np.ones([10,90]), np.ones(10), np.ones(10))\n","print(dldw.shape)\n","print(type(dldb))"],"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["(90,)\n","<class 'numpy.float64'>\n"]}]},{"cell_type":"markdown","metadata":{"id":"XhQXAKd4zGrE"},"source":["### 2.4 Derivative approximation\n","\n","We can check that our derivative is implemented correctly using the finite difference rule. In 1D, the\n","finite difference rule tells us that for small $h$, we should have\n","\n","$$\\frac{f(x+h) - f(x)}{h} \\approx f'(x)$$\n","\n","make sure that $\\frac{\\partial\\mathcal{L}}{\\partial b}$  is implement correctly\n","by comparing the result from `derivative_cost` with the empirical cost derivative computed using the above numerical approximation.\n"]},{"cell_type":"code","metadata":{"id":"SpRTD-fozGrF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708280702435,"user_tz":-120,"elapsed":432,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"4318f9e1-3b4c-486f-a885-5b41aadacf25"},"source":["reload_functions()\n","import ML_DL_Functions2\n","\n","# Your code goes here\n","\n","# Function to compute the analytical derivative of the cost function with respect to bias\n","def analytical_derivative(X, y, t):\n","    dLdw, dLdb = ML_DL_Functions2.derivative_cost(X, y, t)\n","    return dLdb\n","\n","# Function to compute the empirical derivative of the cost function with respect to bias using finite difference rule\n","def empirical_derivative(X, y, t, epsilon=1e-6):\n","    b = 0.0  # Initial bias value\n","    y_plus = ML_DL_Functions2.pred(np.zeros(X.shape[1]), b + epsilon, X)\n","    y_minus = ML_DL_Functions2.pred(np.zeros(X.shape[1]), b - epsilon, X)\n","    L_plus = ML_DL_Functions2.cost(y_plus, t)\n","    L_minus = ML_DL_Functions2.cost(y_minus, t)\n","    return (L_plus - L_minus) / (2 * epsilon)\n","\n","# Example usage\n","X = np.random.randn(100, 90)  # Example input data\n","t = np.random.randint(0, 2, size=100)  # Example true labels (binary)\n","y = ML_DL_Functions2.pred(np.zeros(X.shape[1]), 0.0, X)  # Example predictions\n","\n","# Compute analytical derivative\n","analytical_result = analytical_derivative(X, y, t)\n","\n","# Compute empirical derivative\n","empirical_result = empirical_derivative(X, y, t)\n","\n","# Compute empirical derivative\n","empirical_result = empirical_derivative(X, y, t)\n","\n","r1 = analytical_result\n","r2 = empirical_result\n","print(\"The analytical results is -\", r1)\n","print(\"The algorithm results is - \", r2)"],"execution_count":74,"outputs":[{"output_type":"stream","name":"stdout","text":["The analytical results is - 0.04\n","The algorithm results is -  0.04000000008996807\n"]}]},{"cell_type":"markdown","metadata":{"id":"MTiplTPhzGrF"},"source":["make sure that $\\frac{\\partial\\mathcal{L}}{\\partial {\\bf w}}$  is implement correctly."]},{"cell_type":"code","metadata":{"id":"oVTsHgnPzGrF","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c3ef28c-fff6-4df7-ecb5-a64e24009c34","executionInfo":{"status":"ok","timestamp":1708280728184,"user_tz":-120,"elapsed":500,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["reload_functions()\n","import ML_DL_Functions2\n","import numpy as np\n","\n","# Compute analytical derivative of the cost function with respect to the weights\n","analytical_derivative = ML_DL_Functions2.derivative_cost(X, y, t)[0]\n","\n","# Compute numerical approximation of the derivative using finite difference rule\n","h = 1e-7  # Step size for finite differences\n","w = np.zeros(X.shape[1])  # Example weight vector\n","numerical_derivative = np.zeros_like(w)\n","for i in range(len(w)):\n","    w_plus = w.copy()\n","    w_plus[i] += h\n","    y_plus = ML_DL_Functions2.pred(w_plus, 0.0, X)\n","    L_plus = ML_DL_Functions2.cost(y_plus, t)\n","\n","    w_minus = w.copy()\n","    w_minus[i] -= h\n","    y_minus = ML_DL_Functions2.pred(w_minus, 0.0, X)\n","    L_minus = ML_DL_Functions2.cost(y_minus, t)\n","\n","    numerical_derivative[i] = (L_plus - L_minus) / (2 * h)\n","\n","# Print the results\n","r1 = analytical_derivative\n","r2 = numerical_derivative\n","print(\"The analytical result is -\", r1)\n","print(\"The numerical result is - \", r2)"],"execution_count":76,"outputs":[{"output_type":"stream","name":"stdout","text":["The analytical result is - [-0.04392653 -0.17094196  0.0024962   0.0124585  -0.00711277 -0.00415388\n","  0.08651912 -0.06211943 -0.02601166 -0.00947624  0.02046937  0.01005135\n","  0.13083936 -0.00658304  0.03466959 -0.08135338 -0.03308247  0.0114771\n","  0.03901689  0.0462238  -0.04574524 -0.04336389 -0.07630227 -0.01965408\n","  0.00063889 -0.04227118 -0.06873054  0.05969957 -0.07557205 -0.01500851\n","  0.05593489 -0.06143159 -0.04754292 -0.04943775 -0.06581548  0.01899686\n"," -0.08379418 -0.05301929 -0.03076069 -0.05283535  0.00128271  0.09538158\n","  0.05063841  0.04303076 -0.02056941 -0.03893966 -0.03687689  0.03179959\n"," -0.03426237  0.01467969 -0.0376358  -0.06649469 -0.00716773 -0.03967515\n","  0.06073525 -0.02576996  0.05042928  0.0398671  -0.07577535 -0.00146507\n"," -0.05279495 -0.0257374   0.06634662  0.01696675  0.02787939  0.07461564\n","  0.04088063  0.0812643  -0.04746029 -0.07162935 -0.0426641  -0.02526891\n","  0.06523864 -0.02661568  0.01783896 -0.02713919 -0.11572812  0.08985021\n","  0.01648827  0.04979606  0.02177551  0.06004256  0.04388798  0.03526056\n"," -0.03588783 -0.01441447  0.01672189 -0.04407082  0.0586558  -0.04409518]\n","The numerical result is -  [-0.04392653 -0.17094196  0.0024962   0.0124585  -0.00711277 -0.00415388\n","  0.08651912 -0.06211943 -0.02601166 -0.00947624  0.02046937  0.01005135\n","  0.13083936 -0.00658304  0.03466959 -0.08135338 -0.03308247  0.0114771\n","  0.03901689  0.0462238  -0.04574524 -0.04336389 -0.07630227 -0.01965408\n","  0.0006389  -0.04227118 -0.06873054  0.05969957 -0.07557205 -0.01500851\n","  0.05593489 -0.06143159 -0.04754292 -0.04943775 -0.06581548  0.01899686\n"," -0.08379418 -0.05301929 -0.03076069 -0.05283535  0.00128271  0.09538158\n","  0.05063841  0.04303076 -0.02056941 -0.03893966 -0.03687689  0.0317996\n"," -0.03426237  0.01467968 -0.0376358  -0.06649469 -0.00716773 -0.03967515\n","  0.06073525 -0.02576996  0.05042928  0.0398671  -0.07577535 -0.00146507\n"," -0.05279495 -0.0257374   0.06634663  0.01696675  0.02787939  0.07461564\n","  0.04088063  0.0812643  -0.04746029 -0.07162935 -0.0426641  -0.02526891\n","  0.06523864 -0.02661568  0.01783896 -0.02713919 -0.11572811  0.08985021\n","  0.01648827  0.04979606  0.02177551  0.06004256  0.04388798  0.03526056\n"," -0.03588783 -0.01441447  0.01672189 -0.04407082  0.0586558  -0.04409518]\n"]}]},{"cell_type":"markdown","metadata":{"id":"pgBTPF_2zGrG"},"source":["### 2.5 Gradient descent\n","\n","Now that you have a gradient function that works, we can actually run gradient descent.\n","Complete the following code that will run stochastic: gradient descent training:"]},{"cell_type":"code","metadata":{"id":"nW4DEuuPzGrG","executionInfo":{"status":"ok","timestamp":1708280847591,"user_tz":-120,"elapsed":583,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["reload_functions()\n","import ML_DL_Functions2\n","\n","def run_gradient_descent(w0, b0, train_norm_xs, train_ts, mu=0.1, batch_size=100, max_iters=100):\n","  \"\"\"Return the values of (w, b) after running gradient descent for max_iters.\n","  We use:\n","    - train_norm_xs and train_ts as the training set\n","    - val_norm_xs and val_ts as the test set\n","    - mu as the learning rate\n","    - (w0, b0) as the initial values of (w, b)\n","\n","  Precondition: np.shape(w0) == (90,)\n","                type(b0) == float\n","\n","  Postcondition: np.shape(w) == (90,)\n","                 type(b) == float\n","  \"\"\"\n","  w = w0\n","  b = b0\n","  iter = 0\n","  max_acc = 0\n","  opt_w = w\n","  opt_b = b\n","  cost_list = []\n","  acc_list  = []\n","\n","  while iter < max_iters:\n","    # Shuffle the training set\n","    reindex = np.random.permutation(len(train_xs))\n","    train_norm_xs = train_norm_xs[reindex]\n","    train_ts = train_ts[reindex]\n","\n","    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n","      # minibatch that we are working with:\n","      X = train_norm_xs[i:(i + batch_size)]\n","      t = train_ts[i:(i + batch_size), 0]\n","\n","      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n","      # the \"last\" minibatch\n","      if np.shape(X)[0] != batch_size:\n","        continue\n","\n","      # compute the prediction\n","      y = ML_DL_Functions2.pred(w, b, X)\n","\n","      # calculate gradient(backpropegate)\n","      dw, db = ML_DL_Functions2.derivative_cost(X, y, t)\n","\n","      # update w and b(step)\n","      w -= mu * dw\n","      b -= mu * db\n","\n","      # increment the iteration count\n","      iter += 1\n","\n","      # compute and print the *validation* loss and accuracy\n","      if (iter % 40 == 0):\n","        val_ts_loss = val_ts[:,0]\n","        val_y = ML_DL_Functions2.pred(w, b, val_norm_xs)\n","        val_cost = ML_DL_Functions2.cost(val_y, val_ts_loss)\n","        val_acc = ML_DL_Functions2.get_accuracy(val_y, val_ts)\n","        cost_list.append(val_cost)\n","        acc_list.append(val_acc)\n","\n","        # save the best weights and biases\n","        if val_acc>max_acc:\n","          opt_w = w\n","          opt_b = b\n","          max_val_acc = val_acc\n","\n","        print(\"Iter %d. [Val Acc %.0f%%, Loss %f]\" % (iter, val_acc * 100, val_cost))\n","\n","      if iter >= max_iters:\n","        break\n","\n","\n","  return opt_w, opt_b, cost_list, acc_list"],"execution_count":79,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-MqzT0jGzGrH"},"source":["### 2.6 Running everything!\n","\n","Call `run_gradient_descent` with the weights and biases all initialized to zero. Test your self with different $\\mu$ values and show that if mu is too small then convergance is slow and if mu is too large then the optimization algorithm does not converge. You can add more automation and plot function to help you find the best configuration."]},{"cell_type":"code","metadata":{"id":"tE32Iqo6zGrH","colab":{"base_uri":"https://localhost:8080/","height":908},"executionInfo":{"status":"ok","timestamp":1708281101509,"user_tz":-120,"elapsed":5926,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"77648160-1050-45c4-c060-4a8501e622da"},"source":["reload_functions()\n","import ML_DL_Functions2\n","\n","w0 = np.zeros(90)\n","b0 = np.zeros(1)[0]\n","\n","# Choose values\n","mu = 0.01\n","max_iters = 1000\n","batch_size = 32\n","\n","# Run gradient descent\n","opt_w, opt_b, cost_list, acc_list = run_gradient_descent(w0, b0, train_norm_xs, train_ts, mu, batch_size, max_iters)\n","\n","# Plot the results\n","plt.plot(range(0, max_iters, 40), acc_list, \"r-\")\n","plt.title(\"Classification accuracy per iteration; $\\mu$=\" + str(mu) + \" batch size=\" + str(batch_size))\n","plt.xlabel(\"Iterations\")\n","plt.ylabel(\"Accuracy\")\n","plt.show()\n","\n","# Write your code here\n","# opt_w,opt_b,cost_list,acc_list = run_gradient_descent(w0,b0,mu,batch_size,max_iters)\n","# plt.plot(range(0,max_iters,40),acc_list,\"r-\")\n","# plt.title(\"classification accuracy per iteration; $\\mu$=\"+str(mu)+\" batch size=\"+str(batch_size))\n"],"execution_count":85,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 40. [Val Acc 65%, Loss 0.670161]\n","Iter 80. [Val Acc 67%, Loss 0.657484]\n","Iter 120. [Val Acc 67%, Loss 0.648296]\n","Iter 160. [Val Acc 68%, Loss 0.639889]\n","Iter 200. [Val Acc 68%, Loss 0.633969]\n","Iter 240. [Val Acc 69%, Loss 0.627638]\n","Iter 280. [Val Acc 69%, Loss 0.622733]\n","Iter 320. [Val Acc 70%, Loss 0.617405]\n","Iter 360. [Val Acc 70%, Loss 0.613399]\n","Iter 400. [Val Acc 70%, Loss 0.610355]\n","Iter 440. [Val Acc 70%, Loss 0.607036]\n","Iter 480. [Val Acc 71%, Loss 0.604302]\n","Iter 520. [Val Acc 71%, Loss 0.601881]\n","Iter 560. [Val Acc 71%, Loss 0.599444]\n","Iter 600. [Val Acc 71%, Loss 0.597163]\n","Iter 640. [Val Acc 71%, Loss 0.595088]\n","Iter 680. [Val Acc 71%, Loss 0.593293]\n","Iter 720. [Val Acc 71%, Loss 0.591984]\n","Iter 760. [Val Acc 71%, Loss 0.590502]\n","Iter 800. [Val Acc 71%, Loss 0.589168]\n","Iter 840. [Val Acc 71%, Loss 0.587869]\n","Iter 880. [Val Acc 71%, Loss 0.586567]\n","Iter 920. [Val Acc 72%, Loss 0.585439]\n","Iter 960. [Val Acc 72%, Loss 0.584300]\n","Iter 1000. [Val Acc 72%, Loss 0.583544]\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"EZ5HlDFAzGrH"},"source":["### 2.7 Finding and saving optimal values\n","\n","Find the optimal value of ${\\bf w}$ and $b$ in the means of accuracy using your code. Notice that the choice of $\\mu$ and the batch size are important for this. Run the code below to save these parameters to a file in your google drive directory. submit to the moodle (alongside this file and the functions file) both the\n","\"assignment2_submission_optimal_weights.npy\" file and \"assignment2_submission_optimal_bias.npy\" file."]},{"cell_type":"code","metadata":{"id":"1dFOFSwgzGrI","executionInfo":{"status":"ok","timestamp":1708281140776,"user_tz":-120,"elapsed":655,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"source":["#change these to the optimal weights and biases, leave the name the same\n","np.save(drive_path+\"assignment2_submission_optimal_weights.npy\",opt_w)\n","np.save(drive_path+\"assignment2_submission_optimal_bias.npy\",opt_b)"],"execution_count":87,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KrQqSj2zGrI"},"source":["### 2.8 Results\n","\n","Using the values of `w` and `b` from part 2.7, compute your training accuracy, validation accuracy,\n","and test accuracy. Are there any differences between those three values? If so, why?"]},{"cell_type":"code","metadata":{"id":"fuKw2mLozGrI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708281198680,"user_tz":-120,"elapsed":3789,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"b89a6f03-385f-42bc-ff8e-6d0f8236dccb"},"source":["# Load the optimal values of w and b\n","w = np.load(drive_path + \"assignment2_submission_optimal_weights.npy\")\n","b = np.load(drive_path + \"assignment2_submission_optimal_bias.npy\")\n","\n","# Write your code here\n","\n","# Make predictions on the training set\n","train_predictions = ML_DL_Functions2.pred(w, b, train_norm_xs)\n","train_acc = ML_DL_Functions2.get_accuracy(train_predictions, train_ts)\n","\n","# Make predictions on the validation set\n","val_predictions = ML_DL_Functions2.pred(w, b, val_norm_xs)\n","val_acc = ML_DL_Functions2.get_accuracy(val_predictions, val_ts)\n","\n","# Make predictions on the test set\n","test_predictions = ML_DL_Functions2.pred(w, b, test_norm_xs)\n","test_acc = ML_DL_Functions2.get_accuracy(test_predictions, test_ts)\n","\n","print('train_acc = ', train_acc, ' val_acc = ', val_acc, ' test_acc = ', test_acc)\n"],"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["train_acc =  0.7203582176135745  val_acc =  0.71924  test_acc =  0.7124\n"]}]},{"cell_type":"markdown","source":["### 2.9 Using Pytorch\n","Writing a classifier like this is instructive, and helps you understand what happens when\n","we train a model. However, in practice, we rarely write model building and training code\n","from scratch. Instead, we typically use one of the well-tested libraries available in a package. The following example showes you how this task could have been achieved using the deep learning library, pytorch. The library greatly simplifies the steps needed to create a learning model. Though there is nothing you need to complete in this section we suggest you read this section thoroughly and make sure you understand all the code. In the next assignment you will need to build a deep learning model yourself."],"metadata":{"id":"gO-V4yYACAtF"}},{"cell_type":"markdown","source":["The first step required to use the pytorch module is to create a class which will be our model. in this case we will use a linear layer with a costum size(in your assignment you used a 90,1 linear layer meaning an input size of 90 and an output size of 1). We also add a sigmoid function to restrict the values between 0 and 1.\n","\n","The forward function is called everytime you call the model by name. It is equivalent to the prediction function you wrote but it serves another purpose since it saves all the operations done to the tensor which can then be used to calculate the gradients."],"metadata":{"id":"STfBYRNUGESO"}},{"cell_type":"code","source":["import torch\n","class single_layer(torch.nn.Module):\n","  def __init__(self,input_size,output_size):\n","    super(single_layer,self).__init__()\n","    self.neuron = torch.nn.Linear(input_size,output_size)\n","    self.sigmoid = torch.nn.Sigmoid()\n","\n","  def forward(self,X):\n","    out = self.neuron(X)\n","    out = self.sigmoid(out)\n","    return out\n"],"metadata":{"id":"nFsHov0TCYzU","executionInfo":{"status":"ok","timestamp":1708281229732,"user_tz":-120,"elapsed":5086,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"execution_count":89,"outputs":[]},{"cell_type":"markdown","source":["We can now create a new model.\n","\n","we don't have to write the binary cross entropy loss since it is already written for us(criterion).\n","\n","Also instead of writing the optimzation proccess which in our case was gradient descent(W[n+1] = w[n]-$\\mu$dL/dW) we can use a pre built optimizer(SGD)."],"metadata":{"id":"xD7R_h_DHHxl"}},{"cell_type":"code","source":["model = single_layer(90,1)\n","criterion = torch.nn.BCELoss()\n","optimizer = torch.optim.SGD(model.parameters(),lr = 0.05)"],"metadata":{"id":"tB3DPFRfHB8s","executionInfo":{"status":"ok","timestamp":1708281231398,"user_tz":-120,"elapsed":1668,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"execution_count":90,"outputs":[]},{"cell_type":"markdown","source":["There are a few pre-built training functions but usually the training function is written by hand. this function is similar to the one you wrote in this assignment only we now can use the pre-built tensor functions. Make sure you understood all the differences between the two:"],"metadata":{"id":"4lVhHsxKISRt"}},{"cell_type":"code","source":["import pdb\n","def train_model(model, criterion, optimizer, batch_size=100, max_iters=100):\n","  iter = 0\n","  cost_list = []\n","  acc_list  = []\n","  train_norm_xs_shuff = train_norm_xs\n","  train_ts_shuff = train_ts\n","  val_X_tensor = torch.tensor(val_norm_xs,dtype=torch.float32)\n","  while iter < max_iters:\n","    # shuffle the training set (there is code above for how to do this)\n","    reindex = np.random.permutation(len(train_norm_xs))\n","    train_norm_xs_shuff = train_norm_xs_shuff[reindex]\n","    train_ts_shuff = train_ts_shuff[reindex]\n","\n","    for i in range(0, len(train_norm_xs), batch_size): # iterate over each minibatch\n","      # minibatch that we are working with:\n","      X = train_norm_xs_shuff[i:(i + batch_size)]\n","      t = train_ts_shuff[i:(i + batch_size), 0]\n","\n","      # since len(train_norm_xs) does not divide batch_size evenly, we will skip over\n","      # the \"last\" minibatch\n","      if np.shape(X)[0] != batch_size:\n","        continue\n","      # change the numpy types into torches tensors\n","      X_tensor = torch.tensor(X,dtype=torch.float32)\n","      t_tensor = torch.tensor(t,dtype=torch.float32).unsqueeze(1) # the unsqueeze reshapes (N,) to (N,1)\n","\n","      # a clean up step for PyTorch\n","      optimizer.zero_grad()\n","      # compute the prediction\n","      prediction = model(X_tensor)\n","      # compute the cost/loss\n","      loss = criterion(prediction,t_tensor)\n","      # calculate gradient(backpropegate)\n","      loss.backward()\n","      # update w and b(step)\n","      optimizer.step()\n","      # increment the iteration count\n","      iter += 1\n","      # compute and print the *validation* accuracy\n","      if (iter % 40 == 0):\n","        val_pred = model(val_X_tensor)\n","        val_acc = ML_DL_Functions2.get_accuracy(val_pred,val_ts)\n","        acc_list.append(val_acc)\n","\n","        print(\"Iter %d. [Val Acc %.1f%%]\" % (\n","                iter, val_acc * 100))\n","\n","      if iter >= max_iters:\n","        break\n","\n","\n","  return acc_list\n"],"metadata":{"id":"goyjkL11D5OA","executionInfo":{"status":"ok","timestamp":1708281236714,"user_tz":-120,"elapsed":441,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}}},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":["We can now run the training proccess. You should get pretty similar results to the ones from the model you wrote. Make sure that the results are in the same range and if not fix your model and try again."],"metadata":{"id":"2XTEVQkgJu0i"}},{"cell_type":"code","source":["reload_functions()\n","import ML_DL_Functions2\n","acc_list = train_model(model,criterion,optimizer,100,500)"],"metadata":{"id":"1ajs1myYKl4t","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1708281275220,"user_tz":-120,"elapsed":27704,"user":{"displayName":"Priel Hoffman","userId":"03462811434278478065"}},"outputId":"86d348df-346a-4c25-f1c6-d1a8c0acf931"},"execution_count":92,"outputs":[{"output_type":"stream","name":"stdout","text":["Iter 40. [Val Acc 65.1%]\n","Iter 80. [Val Acc 69.2%]\n","Iter 120. [Val Acc 70.6%]\n","Iter 160. [Val Acc 71.1%]\n","Iter 200. [Val Acc 71.4%]\n","Iter 240. [Val Acc 71.7%]\n","Iter 280. [Val Acc 72.0%]\n","Iter 320. [Val Acc 72.3%]\n","Iter 360. [Val Acc 72.5%]\n","Iter 400. [Val Acc 72.1%]\n","Iter 440. [Val Acc 72.7%]\n","Iter 480. [Val Acc 72.6%]\n"]}]},{"cell_type":"markdown","source":["You can also try to change the model(add layers or change layers) change the optimizer or the hyperparameters and try to improve the validation accuracy. If you want a challenge you can try reach a validation accuracy of 75%"],"metadata":{"id":"XFj6EzWAB7ry"}}]}